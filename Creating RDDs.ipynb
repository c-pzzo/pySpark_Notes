{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edd3a52f",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c8b6772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/06/27 17:47:59 WARN Utils: Your hostname, cesar-GL62M-7RDX resolves to a loopback address: 127.0.1.1; using 192.168.1.155 instead (on interface wlp2s0)\n",
      "22/06/27 17:47:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/06/27 17:48:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/cesar/Python_NBs/pySpark_Notes'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark create RDD example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028ee806",
   "metadata": {},
   "source": [
    "# User-Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "999c8c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User information is ready!\n"
     ]
    }
   ],
   "source": [
    "# For privacy reasons, User Information is located in a .txt file in the same folder\n",
    "# First row contains the user name\n",
    "# Second row contains the password\n",
    "\n",
    "try:\n",
    "    login = pd.read_csv(r'mysql_login.txt', header=None)\n",
    "    mysql_user = login[0][0]\n",
    "    mysql_pwd = login[0][1]\n",
    "    print('User information is ready!')\n",
    "except:\n",
    "    print('Login information is not available!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29845a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a connection. Define parameters\n",
    "mysql_conn = mysql.connector.connect(\n",
    "  host=\"localhost\"\n",
    "  , user = mysql_user\n",
    "  , password = mysql_pwd\n",
    "  , database='HDL_Project'\n",
    ")\n",
    "    \n",
    "def read_sql(mysql_conn, sqlq, rdd = False):\n",
    "    \n",
    "    # -----------------------\n",
    "    # Description:\n",
    "    # Reads MySQL data and transforms into spark RDD object\n",
    "    \n",
    "    # Parameters:\n",
    "    # * sqlq: SQL query.\n",
    "    # * rdd: Binary parameter defining transformation into Spark RDD object\n",
    "    # -----------------------\n",
    "\n",
    "    mysql_cursor = mysql_conn.cursor()\n",
    "\n",
    "    # Create a pandas dataframe\n",
    "    df = pd.read_sql(sqlq, con=mysql_conn)\n",
    "    \n",
    "    mysql_cursor.close()\n",
    "\n",
    "    # Convert Pandas dataframe to spark DataFrame\n",
    "    if rdd == True:\n",
    "        df = spark.createDataFrame(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6006667d",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad552a9",
   "metadata": {},
   "source": [
    "## Creating RDD's using parallelize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7483a5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallelize = spark.sparkContext.parallelize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1349003b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+\n",
      "|station_code| station_name|\n",
      "+------------+-------------+\n",
      "|          SE|   La Pastora|\n",
      "|          NE|  San Nicolás|\n",
      "|          CE|     Obispado|\n",
      "|          NO|  San Bernabé|\n",
      "|          SO|Sta. Catarina|\n",
      "+------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating RDD from input by the user\n",
    "df = parallelize([\n",
    "  ('SE', 'La Pastora')\n",
    ", ('NE', 'San Nicolás')\n",
    ", ('CE', 'Obispado')\n",
    ", ('NO', 'San Bernabé')\n",
    ", ('SO', 'Sta. Catarina')\n",
    ", ('NO2', 'García')\n",
    ", ('NTE', 'Escobedo')\n",
    ", ('NE2', 'Apodaca')\n",
    ", ('SE2', 'Juárez')\n",
    ", ('SO2', 'San Pedro')\n",
    ", ('SE3', 'Cadereyta')\n",
    ", ('SUR', 'Pueblo Serena')\n",
    ", ('NTE2', 'Universidad')\n",
    "]).toDF(['station_code', 'station_name'])\n",
    "\n",
    "# Visualization\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b37f7207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (3, 4), (5, 6), (7, 8), (9, 10)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating RDD from input by the user\n",
    "myData = parallelize([(1,2), (3,4), (5,6), (7,8), (9,10)])\n",
    "\n",
    "# Visualization\n",
    "myData.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1fd20d",
   "metadata": {},
   "source": [
    "## Creating RDD's using createDataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e6dcc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "createDataFrame = spark.createDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9f83736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+\n",
      "|station_code| station_name|\n",
      "+------------+-------------+\n",
      "|          SE|   La Pastora|\n",
      "|          NE|  San Nicolás|\n",
      "|          CE|     Obispado|\n",
      "|          NO|  San Bernabé|\n",
      "|          SO|Sta. Catarina|\n",
      "|         NO2|       García|\n",
      "|         NTE|     Escobedo|\n",
      "|         NE2|      Apodaca|\n",
      "|         SE2|       Juárez|\n",
      "|         SO2|    San Pedro|\n",
      "|         SE3|    Cadereyta|\n",
      "|         SUR|Pueblo Serena|\n",
      "|        NTE2|  Universidad|\n",
      "+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stations = createDataFrame([('SE', 'La Pastora')\n",
    "                            , ('NE', 'San Nicolás')\n",
    "                            , ('CE', 'Obispado')\n",
    "                            , ('NO', 'San Bernabé')\n",
    "                            , ('SO', 'Sta. Catarina')\n",
    "                            , ('NO2', 'García')\n",
    "                            , ('NTE', 'Escobedo')\n",
    "                            , ('NE2', 'Apodaca')\n",
    "                            , ('SE2', 'Juárez')\n",
    "                            , ('SO2', 'San Pedro')\n",
    "                            , ('SE3', 'Cadereyta')\n",
    "                            , ('SUR', 'Pueblo Serena')\n",
    "                            , ('NTE2', 'Universidad')],\n",
    "                        ['station_code', 'station_name'])\n",
    "\n",
    "stations.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de427f6",
   "metadata": {},
   "source": [
    "## Creating RDD's using read() and load() functions\n",
    "spark.read.format(\"csv\").load(\"path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6aa2564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+\n",
      "|station_code| station_name|\n",
      "+------------+-------------+\n",
      "|          SE|   La Pastora|\n",
      "|          NE|  San Nicolás|\n",
      "|          CE|     Obispado|\n",
      "|          NO|  San Bernabé|\n",
      "|          SO|Sta. Catarina|\n",
      "+------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- station_code: string (nullable = true)\n",
      " |-- station_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "read_format = 'com.databricks.spark.csv'\n",
    "file_location = \"Datasets/stations.csv\"\n",
    "\n",
    "\n",
    "df = spark.read.format(read_format).\\\n",
    "                               options(header='true', inferschema='true').\\\n",
    "                load(file_location,header=True)\n",
    "\n",
    "df.show(5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a70765d",
   "metadata": {},
   "source": [
    "## Creating RDD's using spark.read.csv(\"path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b275909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+\n",
      "|station_code| station_name|\n",
      "+------------+-------------+\n",
      "|          SE|   La Pastora|\n",
      "|          NE|  San Nicolás|\n",
      "|          CE|     Obispado|\n",
      "|          NO|  San Bernabé|\n",
      "|          SO|Sta. Catarina|\n",
      "|         NO2|       García|\n",
      "|         NTE|     Escobedo|\n",
      "|         NE2|      Apodaca|\n",
      "|         SE2|       Juárez|\n",
      "|         SO2|    San Pedro|\n",
      "|         SE3|    Cadereyta|\n",
      "|         SUR|Pueblo Serena|\n",
      "|        NTE2|  Universidad|\n",
      "+------------+-------------+\n",
      "\n",
      "root\n",
      " |-- station_code: string (nullable = true)\n",
      " |-- station_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading CSV file\n",
    "df = spark.read.options(header='true', inferschema='true').csv(file_location)\n",
    "\n",
    "# Visualization\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb61b3f6",
   "metadata": {},
   "source": [
    "# Create RDD's from MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d31ffc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cesar/anaconda3/envs/spark_env/lib/python3.9/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+\n",
      "|station_code| station_name|\n",
      "+------------+-------------+\n",
      "|          SE|   La Pastora|\n",
      "|          NE|  San Nicolás|\n",
      "|          CE|     Obispado|\n",
      "|          NO|  San Bernabé|\n",
      "|          SO|Sta. Catarina|\n",
      "|         NO2|       García|\n",
      "|         NTE|     Escobedo|\n",
      "|         NE2|      Apodaca|\n",
      "|         SE2|       Juárez|\n",
      "|         SO2|    San Pedro|\n",
      "|         SE3|    Cadereyta|\n",
      "|         SUR|Pueblo Serena|\n",
      "|        NTE2|  Universidad|\n",
      "+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mysql_cursor = mysql_conn.cursor()\n",
    "\n",
    "# -------------------------------------\n",
    "\n",
    "sqlq = \"SELECT * FROM cat_stations\"\n",
    "\n",
    "# Create a pandas dataframe\n",
    "pdf = pd.read_sql(sqlq, con=mysql_conn)\n",
    "mysql_cursor.close()\n",
    "\n",
    "# Convert Pandas dataframe to spark DataFrame\n",
    "df = spark.createDataFrame(pdf)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1bfb1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cesar/anaconda3/envs/spark_env/lib/python3.9/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+\n",
      "|station_code| station_name|\n",
      "+------------+-------------+\n",
      "|          SE|   La Pastora|\n",
      "|          NE|  San Nicolás|\n",
      "|          CE|     Obispado|\n",
      "|          NO|  San Bernabé|\n",
      "|          SO|Sta. Catarina|\n",
      "|         NO2|       García|\n",
      "|         NTE|     Escobedo|\n",
      "|         NE2|      Apodaca|\n",
      "|         SE2|       Juárez|\n",
      "|         SO2|    San Pedro|\n",
      "|         SE3|    Cadereyta|\n",
      "|         SUR|Pueblo Serena|\n",
      "|        NTE2|  Universidad|\n",
      "+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Previous function has been redesigned as a callable function.\n",
    "sqlq = \"SELECT * FROM cat_stations\"\n",
    "\n",
    "read_sql(mysql_conn, sqlq, rdd = True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4b409f",
   "metadata": {},
   "source": [
    "# Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38baa5ae",
   "metadata": {},
   "source": [
    "## Create RDD from Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3eb12f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'A': [0, 1, 0],\n",
    "     'B': [1, 0, 1],\n",
    "     'C': [1, 0, 0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b5952c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  A|  B|  C|\n",
      "+---+---+---+\n",
      "|  0|  1|  1|\n",
      "|  1|  0|  0|\n",
      "|  0|  1|  0|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tedious for PySpark\n",
    "ds = spark.createDataFrame(np.array(list(d.values())).T.tolist(),list(d.keys()))\n",
    "ds.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbaac2da",
   "metadata": {},
   "source": [
    "## Create RDD from JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b37b762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+------+--------+-------+--------+------------------+\n",
      "|             created|                date|                 gas|precip|pressure|station|    temp|              wind|\n",
      "+--------------------+--------------------+--------------------+------+--------+-------+--------+------------------+\n",
      "|2022-04-15T00:00:...|2022-04-15T12:00:...|{0, 35.0462, 18.2...|17.792|924.4951|     31| 292.054| [-1.1861, 0.1817]|\n",
      "|2022-04-15T00:00:...|2022-04-17T23:00:...|{0, 87.1104, 25.5...|0.0059|921.5762|     31|308.5085|[-4.3963, -1.0718]|\n",
      "|2022-04-15T00:00:...|2022-04-18T00:00:...|{0, 70.8924, 25.3...|   0.0|922.0118|     31|306.3807| [-2.496, -0.7729]|\n",
      "|2022-04-15T00:00:...|2022-04-18T01:00:...|{0, 61.5726, 28.5...|   0.0|922.9371|     31|304.4861|[-2.4889, -0.6556]|\n",
      "|2022-04-15T00:00:...|2022-04-18T02:00:...|{0, 53.8242, 22.574}|   0.0|924.3472|     31|302.8295| [-3.0352, 0.2908]|\n",
      "+--------------------+--------------------+--------------------+------+--------+-------+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.json('Datasets/s31_220415.json').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23cdf1d",
   "metadata": {},
   "source": [
    "# Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3252ed77",
   "metadata": {},
   "source": [
    "Making operations over the RDD is the same as with any Pandas dataframe. \n",
    "(due to space concerns, I'm not displaying an example for each one...) <br>\n",
    "You may:\n",
    "* **df.colums**: Show column names\n",
    "* **df.dtypes**: Show data types for every column\n",
    "* **df.fillna(X)**: Fill NA values with an X value.\n",
    "* **df.col.replace(['A1','B1'],['A2','B2'])**: Replace values for a specific column (col). Mixing data types is not allowed.\n",
    "* [E1]**df.toDF(*new_names)**: Change column names by defining an array with new names\n",
    "* **df.drop(*drop_name)**: Drop data column.\n",
    "* Filter\n",
    "* **df.withColumn()** Add new column\n",
    "* Join\n",
    "* Concatenate columns\n",
    "* GroupBy\n",
    "* Pivot\n",
    "* Window\n",
    "* Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64bf4942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "| A1| B1| C1|\n",
      "+---+---+---+\n",
      "|  0|  1|  1|\n",
      "|  1|  0|  0|\n",
      "|  0|  1|  0|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# E1\n",
    "new_names = ['A1', 'B1', 'C1']\n",
    "ds = ds.toDF(*new_names)\n",
    "ds.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3295f81",
   "metadata": {},
   "source": [
    "# Dictionary of Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2568977",
   "metadata": {},
   "source": [
    "* **Resilient Distributed Dataset (RDD)**: An RDD in Spark is simply an immutable distributed collection of objects sets. Each RDD is split into multiple partitions (similar pattern with smaller sets), which may be computed on different nodes of the cluster.\n",
    "* **Spark Transformations**: Transformations construct a new RDD from a previous one. \n",
    "* **Spark Actions**: Actions compute a result based on an RDD, and either return it to the driver program or save it to an external storage system (e.g., HDFS)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93990eca",
   "metadata": {},
   "source": [
    "# Dictionary of functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1ad0e6",
   "metadata": {},
   "source": [
    "* **parallelize()**: Distribute a local Python collection to form an RDD.\n",
    "https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.SparkContext.parallelize\n",
    "* **createDataFrame()** Creates a DataFrame from an RDD, a list or a pandas.DataFrame.\n",
    "https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.SparkSession.createDataFrame.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a80f58",
   "metadata": {},
   "source": [
    "# Source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f287f29",
   "metadata": {},
   "source": [
    "* 5. Programming with RDDs. <br>\n",
    "https://runawayhorse001.github.io/LearningApacheSpark/rdd.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
