{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd8cfe18",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6832f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/23 19:04:53 WARN Utils: Your hostname, cesar-GL62M-7RDX resolves to a loopback address: 127.0.1.1; using 10.22.162.210 instead (on interface wlp2s0)\n",
      "22/09/23 19:04:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/09/23 19:04:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/09/23 19:04:54 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User information is ready!\n"
     ]
    }
   ],
   "source": [
    "%run \"BasePysparkEnv.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3826fe",
   "metadata": {},
   "source": [
    "# User-Defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46b25527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/cesar/Python_NBs/HDL_Project/Mini HDL/Baseline_ML_Pollution_Concentration_MMA/SparkImplementation'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e543a74e",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c4ce3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+\n",
      "|station_code| station_name|\n",
      "+------------+-------------+\n",
      "|          SE|   La Pastora|\n",
      "|          NE|  San Nicolás|\n",
      "|          CE|     Obispado|\n",
      "|          NO|  San Bernabé|\n",
      "|          SO|Sta. Catarina|\n",
      "|         NO2|       García|\n",
      "|         NTE|     Escobedo|\n",
      "|         NE2|      Apodaca|\n",
      "|         SE2|       Juárez|\n",
      "|         SO2|    San Pedro|\n",
      "|         SE3|    Cadereyta|\n",
      "|         SUR|Pueblo Serena|\n",
      "|        NTE2|  Universidad|\n",
      "+------------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sqlq = \"SELECT * FROM cat_stations\"\n",
    "df = sql_rdd(sqlq)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94819410",
   "metadata": {},
   "source": [
    "# Linear regression examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0aa6d1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e1bda7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/apache/spark/master/data/mllib/sample_linear_regression_data.txt\"\n",
    "file_name = \"sample_linear_regression_data.txt\"\n",
    "from pyspark import SparkFiles\n",
    "spark.sparkContext.addFile(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62302511",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/23 19:05:00 WARN LibSVMFileFormat: 'numFeatures' option not specified, determining the number of features by going though the input. If you know the number in advance, please specify it via 'numFeatures' option to avoid the extra scan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.010541828081257209,0.800325310056095,-0.7845165541420372,2.3679887171421914,0.5010002089857577,1.1222351159753023,-0.2926824398623297,-0.49837174323213046,-0.6035797180675657,0.6725550067187459]\n",
      "Intercept: 0.14592176145232047\n",
      "Coefficient Standard Errors: [0.7950428434287478, 0.8049713176546897, 0.7975916824772489, 0.831264924765992, 0.7945436200517938, 0.8118992572197593, 0.7919506385542777, 0.7973378214726764, 0.8300714999626418, 0.7771333489686802, 0.463930109648428]\n",
      "T Values: [0.013259446542269234, 0.9942283563442595, -0.9836067393599173, 2.8486570846337584, 0.6305509179635714, 1.3822344410293548, -0.36957156874906694, -0.6250446546128239, -0.7271418403049983, 0.8654306337661118, 0.314533931765933]\n",
      "P Values: [0.989426199114056, 0.32060241580811044, 0.3257943227369877, 0.004575078538306521, 0.5286281628105467, 0.16752945248679119, 0.7118614002322872, 0.5322327097421431, 0.467486325282384, 0.3872259825794293, 0.753249430501097]\n",
      "Dispersion: 105.60988356821714\n",
      "Null Deviance: 53229.3654338832\n",
      "Residual Degree Of Freedom Null: 500\n",
      "Deviance: 51748.8429484264\n",
      "Residual Degree Of Freedom: 490\n",
      "AIC: 3769.1895871765314\n",
      "Deviance Residuals: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cesar/anaconda3/envs/hdl/lib/python3.8/site-packages/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|  devianceResiduals|\n",
      "+-------------------+\n",
      "|-10.974359174246889|\n",
      "| 0.8872320138420557|\n",
      "| -4.596541837478908|\n",
      "|-20.411667435019638|\n",
      "|-10.270419345342642|\n",
      "|-6.0156058956799905|\n",
      "|-10.663939415849267|\n",
      "| 2.1153960525024713|\n",
      "|  3.980713237913768|\n",
      "|-17.225218272069533|\n",
      "| -4.611647633532146|\n",
      "|  6.417666940769855|\n",
      "| 11.407137945300537|\n",
      "| -20.70176540467664|\n",
      "| -2.683748540510967|\n",
      "|-16.755494794232536|\n",
      "|  8.154668342638725|\n",
      "|-1.4355057987358848|\n",
      "|  -0.64350586881857|\n",
      "|-1.1380258931683198|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "An example demonstrating generalized linear regression.\n",
    "Run with:\n",
    "bin/spark-submit examples/src/main/python/ml/generalized_linear_regression_example.py\n",
    "\"\"\"\n",
    "from pyspark.sql import SparkSession\n",
    "# $example on$\n",
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "# $example off$\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "\n",
    "# $example on$\n",
    "# Load training data\n",
    "dataset = spark.read.format(\"libsvm\").load(\"file://\"+SparkFiles.get(file_name))\n",
    "\n",
    "glr = GeneralizedLinearRegression(family=\"gaussian\", link=\"identity\", maxIter=10, regParam=0.3)\n",
    "\n",
    "# Fit the model\n",
    "model = glr.fit(dataset)\n",
    "\n",
    "# Print the coefficients and intercept for generalized linear regression model\n",
    "print(\"Coefficients: \" + str(model.coefficients))\n",
    "print(\"Intercept: \" + str(model.intercept))\n",
    "\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "summary = model.summary\n",
    "print(\"Coefficient Standard Errors: \" + str(summary.coefficientStandardErrors))\n",
    "print(\"T Values: \" + str(summary.tValues))\n",
    "print(\"P Values: \" + str(summary.pValues))\n",
    "print(\"Dispersion: \" + str(summary.dispersion))\n",
    "print(\"Null Deviance: \" + str(summary.nullDeviance))\n",
    "print(\"Residual Degree Of Freedom Null: \" + str(summary.residualDegreeOfFreedomNull))\n",
    "print(\"Deviance: \" + str(summary.deviance))\n",
    "print(\"Residual Degree Of Freedom: \" + str(summary.residualDegreeOfFreedom))\n",
    "print(\"AIC: \" + str(summary.aic))\n",
    "print(\"Deviance Residuals: \")\n",
    "summary.residuals().show()\n",
    "# $example off$\n",
    "\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
